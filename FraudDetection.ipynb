{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1MkTfW8zoNa"
   },
   "source": [
    "# 1. Importing the Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTZk4GCHD5Yn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6462,
     "status": "ok",
     "timestamp": 1585648997636,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "PUlnxuZPTK_I",
    "outputId": "4c13d3db-f1b1-460e-ecc8-2cbb3b33951b"
   },
   "outputs": [],
   "source": [
    "#File reading\n",
    "df=pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c592H4f_weKp"
   },
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6427,
     "status": "ok",
     "timestamp": 1585648997637,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "9xWd9qaHS3C_",
    "outputId": "df8c2929-6fff-4d30-b70a-045921a0dd30"
   },
   "outputs": [],
   "source": [
    "#observe the different feature type present in the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6397,
     "status": "ok",
     "timestamp": 1585648997638,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "dftdkeHuPTiV",
    "outputId": "f2daae3b-f728-40db-e442-71f77b24b297"
   },
   "outputs": [],
   "source": [
    "classes=df['Class'].value_counts()\n",
    "normal_share=classes[0]/df['Class'].count()*100\n",
    "fraud_share=classes[1]/df['Class'].count()*100\n",
    "\n",
    "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "fig1, ax1 = plt.subplots(1,2,figsize=(10,4))\n",
    "ax1[0].set_title('Fig 1.1 Class Percentage', fontsize=14)\n",
    "sns.barplot(x=['Normal','Fraud'],y=[normal_share,fraud_share],ax=ax1[0])\n",
    "ax1[1].set_title('Fig 1.2 Class Counts', fontsize=14)\n",
    "sns.barplot(x=['Normal','Fraud'],y=[classes[0],classes[1]],ax=ax1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23318,
     "status": "ok",
     "timestamp": 1585649014595,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "DMgCoqPMY3Os",
    "outputId": "d28f28ae-6eab-4895-fc5a-d8ce0214483e"
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with time\n",
    "fig2, ax2 = plt.subplots(1,2,figsize=(20,4))\n",
    "ax2[0].set_title('Fig 2.1 Class vs Amount', fontsize=14)\n",
    "sns.scatterplot(x=\"Amount\", y=\"Class\", hue=\"Class\",edgecolor=None,data=df,ax=ax2[0])\n",
    "ax2[1].set_title('Fig 2.2 Class vs Time', fontsize=14)\n",
    "sns.scatterplot(x=\"Time\", y=\"Class\", hue=\"Class\",edgecolor=None,data=df,ax=ax2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UTYGxL--huP"
   },
   "source": [
    "### 2.1 Time and Amount Features\n",
    "\n",
    "We notice that the extremely high __Amount__ aren't actually related to the positive class, as one would intutively imagine. There's no boundary possible for amount between Class 0 and 1. Thus we eliminate this <br>\n",
    "The __Time__ feature is just the time elapsed for each transaction from the first one. Since all the transactions are assumed to be independent of each other, we eliminate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7mbXNtvFVjq"
   },
   "outputs": [],
   "source": [
    "df2=df.drop(['Time','Amount'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRJnOMxTnpTW"
   },
   "source": [
    "### 2.2 Visualizaing the Principal Components\n",
    "\n",
    "We now attempt at visualizing classes in the 3D Subspaces , with 3 principal components at a time. This means , we'll visualize V1,V2,V3 followed by V5,V6,V7 and so on, till V18. Our objective is to vizualize if, both the classes are separable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46570,
     "status": "ok",
     "timestamp": 1585649037973,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "NBQHCJIufRf6",
    "outputId": "7dc5d564-9916-499b-ebc3-eec922c3c1e4"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# The list p_comps holds the feature names from V1 to V28\n",
    "p_comps=df2.loc[::,'V1':'V28'].columns\n",
    "\n",
    "# We take features 3 at a time and plot them along the 3 principal components\n",
    "fig3 = plt.figure(figsize=(20,16))\n",
    "fig3.suptitle('Fig 3 Visualising the First 18 Principal Components', fontsize=12)\n",
    "for i in range(0,6):\n",
    "    n=i*3\n",
    "    ax = fig3.add_subplot(2,3,i+1, projection='3d')\n",
    "    g = ax.scatter(df2[p_comps[n]], df2[p_comps[n+1]], df2[p_comps[n+2]], c=df2['Class'], marker='o', depthshade=False, cmap='Paired')\n",
    "    ax.set_xlabel(p_comps[n])\n",
    "    ax.set_ylabel(p_comps[n+1])\n",
    "    ax.set_zlabel(p_comps[n+2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfUqtfZrsF7z"
   },
   "source": [
    "There is indeed some clustering behavior among the two classes. Especially in __V16,V17,V18__ ,  the class-1 points can be seen very clearly. However, we can see no apparent linear boundary in these sub-spaces, though at a higher dimension it may exist.\n",
    "\n",
    "### 2.3 Drop Statistically Insignificant Features\n",
    "\n",
    "We will now attempt a first round elimination of features using statistical methods. For every feature , we can break the data into two populations , each corresponding to the two classes. Then we propose a null hypothesis that $H_0:\\mu_1=\\mu_2$. <br> To form the correct statistical test, we notice the following<br>\n",
    "\n",
    "- Populations Are Normally Distributed. This was mentioned in the problem statement itself.\n",
    "- Populations Have Equal Variances. This is an assumption we make and we note that $\\sigma_1$ and $\\sigma_2$ are unknown. \n",
    "- Samples Are Independent.\n",
    "\n",
    "In view of this, for each feature we perform a two sample t-test. Features which give us large p-values ( >0.05 ) indicate that $H_0$ cannot be rejected and thus can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46541,
     "status": "ok",
     "timestamp": 1585649037973,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "REdp-jo8h06a",
    "outputId": "6ec47090-76ca-4c33-d2f1-ebab08189859"
   },
   "outputs": [],
   "source": [
    "# We'll add feature names to be dropped.\n",
    "dropped_features=[]\n",
    "\n",
    "for feature in p_comps:\n",
    "    # Create the two samples X1 and X2 based on class they belong to\n",
    "    x1=df[df['Class']==1][feature]\n",
    "    x2=df[df['Class']==0][feature]\n",
    "\n",
    "    # Calculate the t-statistic and p-value\n",
    "    t_stat,p_val=stats.ttest_ind(x1, x2, equal_var = True)\n",
    "\n",
    "  # Is p-value in acceptance region ?\n",
    "    if p_val>0.05:\n",
    "        print('p value is {0:2.4f}'.format(p_val),',Cannot reject H0 for',feature)\n",
    "        # Cannot reject H0, so feature can be dropped.\n",
    "        dropped_features.append(feature)\n",
    "\n",
    "# We can drop the high p-value features\n",
    "df3=df2.drop(dropped_features,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tg--jKOVipli"
   },
   "source": [
    "### 2.4 Pre-processing - Power Transformation and Scaling\n",
    "\n",
    "In this step we'll check the skewness of the data and deal with it using power transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1g5bE5m1UiA"
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA,QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score,GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix,roc_curve,roc_auc_score,auc,confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p42Y6yFf16dc"
   },
   "source": [
    "We'll split the data into a training set and a __hold out test__ set in 80:20 Ratio. Since minority class is very low, we'll make sure to us stratify the train test split.<br> Later, we'll run a 4-fold Stratified Validation on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ML_lMy1WgDdd"
   },
   "outputs": [],
   "source": [
    "X=df3.drop('Class',axis=1)\n",
    "y=df3['Class']\n",
    "\n",
    "# The X_test and y_test will be kept completely isolated and use it as hold out set.\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.20, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46494,
     "status": "ok",
     "timestamp": 1585649037975,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "Iu0UNNn_HTRg",
    "outputId": "2bc159f2-925e-4db8-bfb6-bc8078292528"
   },
   "outputs": [],
   "source": [
    "print('Percentage of Minority Class in train Set {:.4f}%'.format(100*y_train.value_counts()[1]/y_train.shape[0]))\n",
    "print('Percentage of Minority Class in hold out test Set {:.4f}%'.format(100*y_test.value_counts()[1]/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmCUf1sdIqEs"
   },
   "source": [
    "We'll  use power transformation to eliminate any skewness. After fitting a power-transformer on the train set, we'll transform the test set as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78oznll3iwlh"
   },
   "outputs": [],
   "source": [
    "# Fit a power transformer on the train set and use it to transform the test set \n",
    "pt=PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "pt.fit(X_train)\n",
    "X_train=pd.DataFrame(pt.transform(X_train),columns=X_train.columns)\n",
    "X_test=pd.DataFrame(pt.transform(X_test),columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9XQBqxy7rzF"
   },
   "outputs": [],
   "source": [
    "# Fit a min-max scaler on the train set and use it to transform the test set \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=pd.DataFrame(scaler.transform(X_train),columns=X_train.columns)\n",
    "X_test=pd.DataFrame(scaler.transform(X_test),columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwedQEMus4H6"
   },
   "source": [
    "### 2.5 Oversampling of Minority Class\n",
    "\n",
    "In this section we see the impact of the Oversampling techniques, which we'll later use in the model building. We apply SMOTE and ADASYN respectively to the train set. Also recall, that the principal components __V16,V17 and V18__ could show the two classes most distinctly. So we use the same components to visualize the oversampled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7t8V7ABMtGaR"
   },
   "outputs": [],
   "source": [
    "sm=SMOTE()\n",
    "ad=ADASYN()\n",
    "X_smt, y_smt = sm.fit_resample(X_train, y_train)  \n",
    "X_ads, y_ads = ad.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minority Classes with and without Oversampling on Training Set')\n",
    "print('No. of positive Classes is {} at {:0.4f} %'.format(y_train.sum(),100*y_train.sum()/y_train.shape[0]))\n",
    "print('No. of positive Classes with SMOTE is {} at {:0.2f} %'.format(y_ads.sum(),100*y_ads.sum()/y_ads.shape[0]))\n",
    "print('No. of positive Classes with ADASYN is {} at {:0.2f} %'.format(y_smt.sum(),100*y_smt.sum()/y_smt.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 76968,
     "status": "ok",
     "timestamp": 1585649068499,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "pE4HCEhuuGeU",
    "outputId": "b31df55f-5929-48f4-d1b7-8c9cd24d88b3"
   },
   "outputs": [],
   "source": [
    "fig4 = plt.figure(figsize=(20,6))\n",
    "\n",
    "# Plot the train set without oversampling\n",
    "ax4 = fig4.add_subplot(131, projection='3d')\n",
    "ax4.scatter(X_train['V16'], X_train['V17'], X_train['V18'], c=y_train, marker='o', cmap='Paired',alpha=0.1)\n",
    "ax4.set_xlabel('V16')\n",
    "ax4.set_ylabel('V17')\n",
    "ax4.set_zlabel('V18')\n",
    "ax4.set_title('Fig 4.1 No Oversampling', fontsize=14)\n",
    "\n",
    "# Plot the SMOTE transformed train set , To reduce processing time, we plot every alternate point\n",
    "ax4 = fig4.add_subplot(132, projection='3d')\n",
    "# The ::2 in the slicers select every alternate points\n",
    "ax4.scatter(X_smt.iloc[::2,15], X_smt.iloc[::2,16], X_smt.iloc[::2,17], c=y_smt[::2], marker='o', cmap='Paired',alpha=0.05)\n",
    "ax4.set_xlabel('V16')\n",
    "ax4.set_ylabel('V17')\n",
    "ax4.set_zlabel('V18')\n",
    "ax4.set_title('Fig 4.2 SMOTE Oversampling', fontsize=14) \n",
    "\n",
    "# Plot the ADASYB transformed train set. To reduce processing time, we plot every alternate point\n",
    "ax4 = fig4.add_subplot(133, projection='3d')\n",
    "# The ::2 in the slicers select every alternate points\n",
    "ax4.scatter(X_ads.iloc[::2,15], X_ads.iloc[::2,16], X_ads.iloc[::2,17], c=y_ads[::2], marker='o', cmap='Paired',alpha=0.05)\n",
    "ax4.set_xlabel('V16')\n",
    "ax4.set_ylabel('V17')\n",
    "ax4.set_zlabel('V18')\n",
    "ax4.set_title('Fig 4.3 ADASYN Oversampling', fontsize=14)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSG2iKyC9mdm"
   },
   "source": [
    "# 3. Building the Classifier\n",
    "\n",
    "We will reach the final model in incremental steps. We start by establishing the baseline with a Logistic regression Model using a workflow with pipeline<br>\n",
    "\n",
    "- Use a __stratified 4 fold__ cross validation method on the transformed and scaled train set. The hold out test set will be completely isolated.\n",
    "- The minority class in each fold is upsampled using either __SMOTE__ or __ADASYN__.\n",
    "- We'll make our choice of the oversampling method in the baseline model itself. Once this choice is made, we'll use it on all future models.\n",
    "- Fit Chosen Classifer on the test data using the GridSearchCV parameter grid.\n",
    "- Find the best hyperparameter and run the model on the __hold out__ test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqrAA07SOl2a"
   },
   "source": [
    "### 3.1 Logistic Regression\n",
    "\n",
    "At this stage we have scaled train and hold out test sets with 80% and 20% of data. This means, there are ~100 Class-1's in Hold Out test Set and ~400 in test set. Thus, if we run a 4-fold stratified cross validation on the train set, we'll have 100 Class-1's in every fold.<br>\n",
    "We are now set to build the first baseline model using Logistic Regression. \n",
    "\n",
    "- The optimum regularization __L1 or L2__ will be found using __GridSearchCV__.\n",
    "- The regularization strength __C__ is a another hyperparameter, that we'll choose from 1,10,100 respectively.\n",
    "- We'll also choose the best Oversampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4PaE48njdiA"
   },
   "outputs": [],
   "source": [
    "#  We have 80% of data in train set, so we use a 4 fold cross val to have roughly 100 Class-1 Samples\n",
    "skf = StratifiedKFold(n_splits=4, shuffle = True, random_state = 42)\n",
    "\n",
    "# We shall define 3 pipelines one without oversamplig followed by one each for SMOTE and ADASYN\n",
    "my_pipe=Pipeline([('logisticregression',LogisticRegression())])\n",
    "\n",
    "# The next two pipelines are one each for SMOTE and ADASYN\n",
    "# Our pipeline my_pipe, shall first oversample the minority class and then fit the logistic regression\n",
    "\n",
    "my_smote_pipe=Pipeline([('smote',SMOTE(random_state = 42)),('logisticregression',LogisticRegression())])\n",
    "my_asyn_pipe=Pipeline([('smote',ADASYN(random_state = 42)),('logisticregression',LogisticRegression())])\n",
    "\n",
    "\n",
    "# In this case we just have two hyperparameters , C and a choice of Ridge vs Lasso.\n",
    "# The  parameter grid is in my_grid. The regulrization strength is to be chosen from 10^-3 to 10^3\n",
    "params = {\"C\":[1,10,100], \"penalty\":[\"l1\",\"l2\"]}\n",
    "my_grid={'logisticregression__' + key: params[key] for key in params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 515562,
     "status": "ok",
     "timestamp": 1585649507131,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "UNBYjFmw65Nc",
    "outputId": "75b3f8b5-a2ec-4f33-ae91-48b790b80b89"
   },
   "outputs": [],
   "source": [
    "# The GridsearchCV runs on the my_pipe and my_grid\n",
    "# We define 3 classifiers for the three pipelines, so that we can compare there performance\n",
    "logistic_clf = GridSearchCV(my_pipe, param_grid = my_grid, cv = skf, scoring='roc_auc',verbose = 1,n_jobs=-1,return_train_score=False)\n",
    "logistic_clf_smote = GridSearchCV(my_smote_pipe, param_grid = my_grid, cv = skf, scoring='roc_auc',verbose = 1,n_jobs=-1,return_train_score=False)\n",
    "logistic_clf_asyn = GridSearchCV(my_asyn_pipe, param_grid = my_grid, cv = skf, scoring='roc_auc',verbose = 1,n_jobs=-1,return_train_score=False)\n",
    "\n",
    "best_logistic=logistic_clf.fit(X_train, y_train)\n",
    "best_logistic_smote=logistic_clf_smote.fit(X_train, y_train)\n",
    "best_logistic_asyn=logistic_clf_asyn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "059MyqORKvjD"
   },
   "source": [
    "We have three classifiers at hand , so we can check their performance on the hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 515628,
     "status": "ok",
     "timestamp": 1585649507215,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "iKyjwWBFH7T1",
    "outputId": "c3b93bdc-5b6c-4953-ed9a-a92727b29fe2"
   },
   "outputs": [],
   "source": [
    "# Run the best model to predict the test set\n",
    "y_test_predict = best_logistic.predict(X_test)\n",
    "lr_auc_score=metrics.roc_auc_score(y_test, y_test_predict)\n",
    "print('Logistic Regression AUC on Test Set No Oversampling {:.4f}'.format(lr_auc_score))\n",
    "y_test_predict = best_logistic_smote.predict(X_test)\n",
    "lr_auc_score=metrics.roc_auc_score(y_test, y_test_predict)\n",
    "print('Logistic Regression AUC on Test Set with SMOTE {:.4f}'.format(lr_auc_score))\n",
    "y_test_predict = best_logistic_asyn.predict(X_test)\n",
    "lr_auc_score=metrics.roc_auc_score(y_test, y_test_predict)\n",
    "print('Logistic Regression AUC on Test Set with ADASYN {:.4f}'.format(lr_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Wjxsd1YXNjF"
   },
   "source": [
    "Undoubtedly oversampling __gave us a gain of >10%__. However, among the two oversampling techniques SMOTE is best. Thus, this'll be our chosen oversampling method.\n",
    "\n",
    "#### 3.1.1 Baseline Model Parameters\n",
    "\n",
    "We can check the hyperparameters for the best model generated by SMOTE oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 515617,
     "status": "ok",
     "timestamp": 1585649507225,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "ESbmsAccweJX",
    "outputId": "71e14095-f081-4114-9f4a-5f0cda1c88b6"
   },
   "outputs": [],
   "source": [
    "print('Best Hyperparameters')\n",
    "print('C ',best_logistic.best_params_['logisticregression__C'])\n",
    "print('Regularization ',best_logistic_smote.best_params_['logisticregression__penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZkmsnHmhhk9"
   },
   "source": [
    "That's actually very good model. The AUC to beat now is 0.9483 using other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GElx9ME3WYGp"
   },
   "outputs": [],
   "source": [
    "# Save the Logistic Model without SMOTE for later use\n",
    "filename = 'best_logistic_no_smote.sav'\n",
    "pickle.dump(best_logistic, open(filename, 'wb'))\n",
    "\n",
    "# Save the Logistic Model for later use\n",
    "filename = 'best_logistic_smote.sav'\n",
    "pickle.dump(best_logistic_smote, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIyfObbghWSo"
   },
   "source": [
    "### 3.2 LDA and QDA\n",
    "\n",
    "Since the problem statement mentions that all the features are Gaussian in nature, we can make the assumption that p(x | y) is a Gaussian distribution.This gives us the opportunity to attempt __LDA__ and __QDA__. We attempt both the models in a pipeline  and see which one fares better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553446,
     "status": "ok",
     "timestamp": 1585649545087,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "F24q3ySWn3xI",
    "outputId": "dc16ff7b-a42b-4e47-f9b9-66e14096cf7d"
   },
   "outputs": [],
   "source": [
    "my_pipe=Pipeline([('smote',SMOTE(random_state = 42)),('classifier',QDA())])\n",
    "lda_grid = [\n",
    "    {'classifier' : [QDA()]},\n",
    "    {'classifier' : [LDA()],\n",
    "     'classifier__solver' : ['svd', 'lsqr','eigen']}    \n",
    "]\n",
    "lda_clf = GridSearchCV(my_pipe, param_grid = lda_grid, cv = skf, scoring='roc_auc',verbose = 1,n_jobs=-1,return_train_score=False)\n",
    "best_lda = lda_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553441,
     "status": "ok",
     "timestamp": 1585649545120,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "6NRZ6-YFCRJK",
    "outputId": "b2cb981e-a89e-4435-8551-b56a8e47221d"
   },
   "outputs": [],
   "source": [
    "# Save the LDA Model for later use\n",
    "filename = 'best_lda.sav'\n",
    "pickle.dump(best_lda, open(filename, 'wb'))\n",
    "best_lda.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among LDA and QDA , the Gridsearch has rated the former as better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553418,
     "status": "ok",
     "timestamp": 1585649545121,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "7KCOv8iOBrQM",
    "outputId": "77ed65bf-fcd1-4882-dbd8-e23068877ef9"
   },
   "outputs": [],
   "source": [
    "y_test_predict = best_lda.predict(X_test)\n",
    "lda_auc_score=metrics.roc_auc_score(y_test, y_test_predict)\n",
    "print('LDA AUC on Test Set {:.4f}'.format(lda_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmQYb1jx3Mgl"
   },
   "source": [
    "That's somewhat less compared to Logistic regression. Financially, this may translate to a huge loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGClSj4mAhCO"
   },
   "source": [
    "### 3.3 KNN Model\n",
    "\n",
    "We now switch to KNN Model. Since our preferred oversampling is __SMOTE__ , we'll us the pipeline __my_smote_pipe__ for fitting the __GridSearchCV__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HptAZqyU17Bk"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9namcsbkz7kd"
   },
   "outputs": [],
   "source": [
    "my_knn_pipe=Pipeline([('smote',SMOTE(random_state = 42)),('knn', KNeighborsClassifier(algorithm='auto',n_jobs=-1))])\n",
    "knn_grid = {\n",
    "        'knn__n_neighbors': [20,30,40],\n",
    "        'knn__weights':['distance','uniform']\n",
    "        }\n",
    "\n",
    "knn_clf = GridSearchCV(my_knn_pipe, param_grid = knn_grid, cv = skf, scoring='roc_auc',verbose = 1,n_jobs=-1,return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3794671,
     "status": "ok",
     "timestamp": 1585652786435,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "dds7E69-3l2G",
    "outputId": "1b79e1db-1e00-4d92-d9bf-8c977d7be21d"
   },
   "outputs": [],
   "source": [
    "best_knn = knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3968130,
     "status": "ok",
     "timestamp": 1585652959912,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "fItqO7Ni4_Hr",
    "outputId": "769d34b9-357a-4c12-981b-f7715bb88cb3"
   },
   "outputs": [],
   "source": [
    "y_test_predict = best_knn.predict(X_test)\n",
    "knn_auc_score=metrics.roc_auc_score(y_test, y_test_predict)\n",
    "print('LDA AUC on Test Set {:.4f}'.format(knn_auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1585653447361,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "3n60tLX2BlvM",
    "outputId": "1dde0050-d0eb-4f72-ba2f-cf2e11af2430"
   },
   "outputs": [],
   "source": [
    "# Save the KNN Model for later use\n",
    "filename = 'best_knn.sav'\n",
    "pickle.dump(best_knn, open(filename, 'wb'))\n",
    "best_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost Model\n",
    "\n",
    "Despite multiple attempts , we weren't able to put SMOTE in the pipeline and search the best Hyper-parameters. The time taken was abnormally large. Thus after trying on local work-station, Nimblebox and Google Colab, we simply put the best parameters in the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xboost=xgb.XGBClassifier(tree_method='exact',max_bin=16,random_state=42,nthread=2,n_gpus=1, objective='binary:logistic')\n",
    "my_xgb_pipe=Pipeline([('smote',SMOTE(random_state = 42)),\n",
    "                      ('xgb', xgb.XGBClassifier(tree_method='exact',max_bin=16,random_state=42,nthread=2, objective='binary:logistic'))])\n",
    "\n",
    "# Due to abnormally large training times , we just retain the best parameters found from\n",
    "# Nimblebox, Colaband and local workstation.\n",
    "# Our Hyperparameter grid for XGBoost\n",
    "gb_grid = {\n",
    "        'xgb__n_estimators':[400],\n",
    "        'xgb__max_depth': [6],\n",
    "        'xgb__learning_rate':[0.001],\n",
    "        'xgb__subsample':[1],\n",
    "        'xgb__colsample_bytree':[1.0]\n",
    "        }\n",
    "\n",
    "xgb_clf = GridSearchCV(my_xgb_pipe, param_grid = gb_grid, cv = skf,\n",
    "                       scoring='roc_auc',verbose = 10,n_jobs=4,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = best_xgb.predict(X_test)\n",
    "xgb_auc_score=metrics.roc_auc_score(y_test, y_test_predict)\n",
    "print('XGB AUC on Test Set {:.4f}'.format(xgb_auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model=best_xgb.best_estimator_\n",
    "#pickle.dump(best_xgb_model, open(\"best_xgb.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparing all Models\n",
    "\n",
    "We now have 5 models. The first one without oversampling and the remaining with SMOTE. \n",
    "\n",
    "- Logistic Regression without Oversampling\n",
    "- Logistic Regression with SMOTE __( We established that SMOTE was performing better than ADASYN )__\n",
    "- LDA with SMOTE\n",
    "- KNN with SMOTE\n",
    "- XGBOOST with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553400,
     "status": "ok",
     "timestamp": 1585649545122,
     "user": {
      "displayName": "Arindam Dey",
      "photoUrl": "",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "rh5vhXdH4M-M",
    "outputId": "8e50f8f1-fdd8-488e-b6c5-065c7ad0da0d"
   },
   "outputs": [],
   "source": [
    "fig5,ax5 = plt.subplots(1,1,figsize=(8,8))\n",
    "ax5.set_title('Fig 5 Model Comparison on Hold Out Test Set', fontsize=14)\n",
    "ax5 = sns.lineplot(x=[0,1], y=[0,1],label='1. Dumb Model -- 0.5',linewidth= 3,color='k')\n",
    "ax5.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "# We make a list of the model and model names built so far\n",
    "models=[best_logistic,best_logistic_smote,best_lda,best_knn,best_xgb_model]\n",
    "model_names=['Logistic w/o SMOTE','Logistic','LDA','KNN','XGBOOST']\n",
    "colors=['g','r','b','k','c']\n",
    "\n",
    "# We simply loop through the models and draw the ROC-AUC curve for each.\n",
    "for i in range(0,5):\n",
    "    # Predict the class probabilities for the ith model\n",
    "    y_proba=models[i].predict_proba(X_test)\n",
    "\n",
    "    # Keep only the positive class probabilities\n",
    "    y_proba = y_proba[:, 1]\n",
    "\n",
    "    # Calculated the predicted class to get AUC\n",
    "    y_test_predict = models[i].predict(X_test)\n",
    "    auc_score=metrics.roc_auc_score(y_test, y_test_predict)\n",
    "\n",
    "    # Generate the FPR and TPR for the ROC Curve\n",
    "    fpr, tpr, thds = roc_curve(y_test, y_proba)\n",
    "    # To save processing time we'll plot the curve every after 20 points.\n",
    "    label=str(i+2)+'. {} -- {:.3f}'.format(model_names[i],auc_score)\n",
    "    ax5 = sns.lineplot(x=fpr[::20], y=tpr[::20],label=label,linewidth= 3,color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cost Benefit Analysis on Hold Out test Set\n",
    "\n",
    "We were expecting that XGBOOST to be best model, however due to awfully long tuning times , we couldn't continue hyperparameter tuning. So we choose Logistic Regression that we built using SMOTE. We will now choose an appropriate threshold. <br> We take the best Logistic Model from our earlier used gridsearchcv object and plot the ROC-AUC curve along with the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best estimator from the best_logistic_smote grid-search-cv object\n",
    "best_of_all=best_logistic_smote.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the class prediction probabilities and retain only class-1  probabilities\n",
    "y_proba=best_of_all.predict_proba(X_test)\n",
    "y_proba = y_proba[:, 1]\n",
    "\n",
    "# Evaluate the metrics for various thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_proba)\n",
    "# compute AUC\n",
    "roc_auc = auc(fpr, tpr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Choosing the Correct Threshold\n",
    "\n",
    "The plot below will help us to estimate the correct threshold, so as to arrive at a correct FPR-TPR combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Threshold on the ROC Curve\n",
    "fig6 = plt.figure(figsize=(9,6))\n",
    "\n",
    "ax6 = fig6.add_subplot(111)\n",
    "ax6.set_title('Fig 6 Choosing the Correct Threshold with Hold Out ROC-AUC {:0.3f}'.format(roc_auc), fontsize=14)\n",
    "ax6.set_ylabel('TPR and Threshold')\n",
    "ax6.set_xlabel('FPR')\n",
    " \n",
    "# Plot the AUC Curve\n",
    "ax6=sns.lineplot(fpr[1::], thresholds[1::], markeredgecolor='r',linestyle='dashed', color='r',label='Thresholds')\n",
    "# Plot the Thresholds Curve\n",
    "ax6=sns.lineplot(x=fpr[1::], y=tpr[1::],linewidth= 3,color='b',label='FPR vs TPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, choosing the correct threshold is a business decision. Aiming a large TPR would also lead to high number of false negatives as well. Thus the Call Costs @ Rs10 may overshadow the savings.\n",
    "We choose a threshold of __0.75__ so that __TPR = 89%__ and __FPR=1%__.<br>\n",
    "\n",
    "### 5.2 Confusion Matrix\n",
    "\n",
    "Having chosen the threshold, we need to convert the prediction probabilities to predicted classes and then plot the confusion matrix. __We recall, that in the hold out test set, there are approximately 100  Positive cases__. Let us see, how much of these are we able to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Prediction probabilities to class Predictions\n",
    "y_pred=np.where(y_proba>0.75,1,0)\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "cm = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "df_cm = pd.DataFrame(cm, index = [0,1],columns = [0,1])\n",
    "sns.heatmap(df_cm, annot=True,cbar=False)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Net Savings - Cost Benefit Analysis\n",
    "\n",
    "Thus , among the 98 positive cases we have successfully captured 87. The flipside is , that almost 630 false cases are also triggered.<br>\n",
    "\n",
    "We will now find out the actual savings/losses.\n",
    "\n",
    "- Savings = sum of transactions for true positives\n",
    "- Losses = sum of transactaions for false negatives\n",
    "- Costs = Calls Costs @ Rs10 for True Positives and False Positives\n",
    "\n",
    "We do these calculations on the Hold Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indices from the labels of test set\n",
    "ids=list(y_test.index)\n",
    "\n",
    "# Extract the Amount corresponding to these indices from the original data\n",
    "transactions=df.loc[ids,'Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a dataframe containing the amount , the ground truths and the predicted labels\n",
    "# from those indices\n",
    "save_df=pd.DataFrame({'Transactions':transactions,'Ground Truth':y_test,\n",
    "                      'Predicted':y_pred})#.to_csv('Predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Sum the amounts for false negatives and call it loss\n",
    "loss=savings[(save_df['Predicted']==0) & (save_df['Ground Truth']==1)]['Transactions'].sum()\n",
    "\n",
    "# We sum the amounts for the true positives and call then savings\n",
    "savings=savings[(save_df['Predicted']==1) & (save_df['Ground Truth']==1)]['Transactions'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amounts corresponding to ground truths with positive classes\n",
    "fraud_amounts=save_df[save_df['Ground Truth']==1]['Transactions'].sum()\n",
    "print('Fraud Amount is {}'.format(fraud_amounts))\n",
    "\n",
    "# This is our savings amount\n",
    "print('Savings {:0.3f}'.format(savings-((cm[0,1]+cm[1,1])*10+loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have minimised the losses to Rs 208.00. Without using the model the loss would have been Rs10644."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMcMcUqbaJ0UL/VCc6qzrPf",
   "collapsed_sections": [],
   "mount_file_id": "1mrqDAuyfO33urgBZwf2bcMfmY9uZ72-9",
   "name": "Credit Card Fraud V2.ipynb",
   "provenance": [
    {
     "file_id": "1I8-Kr7ZX2ZmZOPyhiu6hY50ywrTAWXD_",
     "timestamp": 1584863475999
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
